{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/motatasher/miniconda3/envs/HS4/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "# from plotly_resampler import FigureWidgetResampler\n",
    "\n",
    "import os\n",
    "import typing as tp\n",
    "import numpy.typing as npt\n",
    "\n",
    "import wandb\n",
    "import lightning as L\n",
    "from lightning.pytorch.utilities.types import STEP_OUTPUT\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "# os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Map:\n",
    "    def __init__(self, cells: npt.NDArray):\n",
    "        self._width = cells.shape[1]\n",
    "        self._height = cells.shape[0]\n",
    "        self._cells = cells\n",
    "\n",
    "    def in_bounds(self, i: int, j: int) -> bool:\n",
    "        return 0 <= j < self._width and 0 <= i < self._height\n",
    "\n",
    "    def traversable(self, i: int, j: int) -> bool:\n",
    "        return not self._cells[i, j]\n",
    "\n",
    "    def get_neighbors(self, i: int, j: int) -> tp.List[tp.Tuple[int, int]]:\n",
    "        neighbors = []\n",
    "        delta = ((0, 1), (1, 0), (0, -1), (-1, 0))\n",
    "        for dx, dy in delta:\n",
    "            ni, nj = i + dx, j + dy\n",
    "            if self.in_bounds(ni, nj) and self.traversable(ni, nj):\n",
    "                neighbors.append((ni, nj))\n",
    "        \n",
    "        delta = ((1, 1), (1, -1), (-1, 1), (-1, -1))\n",
    "        for dx, dy in delta:\n",
    "            ni, nj = i + dx, j + dy\n",
    "            if self.in_bounds(ni, nj) and self.traversable(ni, nj) \\\n",
    "                and self.traversable(ni, j) and self.traversable(i, nj):\n",
    "                neighbors.append((ni, nj))\n",
    "\n",
    "        return neighbors\n",
    "\n",
    "    def get_size(self) -> tp.Tuple[int, int]:\n",
    "        return self._height, self._width\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(\n",
    "        self,\n",
    "        i: int,\n",
    "        j: int,\n",
    "        g: tp.Union[float, int] = 0,\n",
    "        h: tp.Union[float, int] = 0,\n",
    "        f: tp.Optional[tp.Union[float, int]] = None,\n",
    "        parent: \"Node\" = None,\n",
    "    ):\n",
    "        \n",
    "        self.i = i\n",
    "        self.j = j\n",
    "        self.g = g\n",
    "        self.h = h\n",
    "        if f is None:\n",
    "            self.f = self.g + h\n",
    "        else:\n",
    "            self.f = f\n",
    "        self.parent = parent\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        \n",
    "        return self.i == other.i and self.j == other.j\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        \n",
    "        return self.f < other.f\n",
    "\n",
    "    def __hash__(self):\n",
    "        return (self.i, self.j).__hash__()\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"{self.i}, {self.j}, {self.h}, {self.g}, {self.f}\"\n",
    "        \n",
    "\n",
    "def convert_string_to_cells(cell_str: str) -> npt.NDArray:\n",
    "    lines = cell_str.replace(\" \", \"\").split(\"\\n\")\n",
    "\n",
    "    cells = np.array(\n",
    "        [[0 if char == \".\" else 1 for char in line] for line in lines if line],\n",
    "        dtype=np.int8,\n",
    "    )\n",
    "    return cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def draw_rectangle(draw, node, scale, color):\n",
    "    draw.rectangle(\n",
    "        (\n",
    "            node.j * scale,\n",
    "            node.i * scale,\n",
    "            (node.j + 1) * scale - 1,\n",
    "            (node.i + 1) * scale - 1,\n",
    "        ),\n",
    "        fill=color,\n",
    "        width=0,\n",
    "    )\n",
    "\n",
    "\n",
    "def draw(\n",
    "    grid_map: Map,\n",
    "    start: tp.Optional[Node] = None,\n",
    "    goal: tp.Optional[Node] = None,\n",
    "    path: tp.Optional[tp.Iterable[Node]] = None,\n",
    "    nodes_discovered: tp.Optional[tp.Iterable[Node]] = None,\n",
    "    nodes_expanded: tp.Optional[tp.Iterable[Node]] = None,\n",
    "    nodes_reexpanded: tp.Optional[tp.Iterable[Node]] = None,\n",
    "):\n",
    "    scale = 5\n",
    "    height, width = grid_map.get_size()\n",
    "    im = Image.new(\"RGB\", (width * scale, height * scale), color=\"white\")\n",
    "    draw = ImageDraw.Draw(im)\n",
    "\n",
    "    for i in range(height):\n",
    "        for j in range(width):\n",
    "            if not grid_map.traversable(i, j):\n",
    "                draw.rectangle(\n",
    "                    (j * scale, i * scale, (j + 1) * scale - 1, (i + 1) * scale - 1),\n",
    "                    fill=(70, 80, 80),\n",
    "                )\n",
    "\n",
    "    node_colors = [\n",
    "        (nodes_discovered, (213, 219, 219)),\n",
    "        (nodes_expanded, (131, 145, 146)),\n",
    "        (nodes_reexpanded, (255, 145, 146)),\n",
    "    ]\n",
    "\n",
    "    for nodes, color in node_colors:\n",
    "        if nodes is not None:\n",
    "            for node in nodes:\n",
    "                draw_rectangle(draw, node, scale, color)\n",
    "\n",
    "    if path is not None:\n",
    "        for step in path:\n",
    "            color = (52, 152, 219) if grid_map.traversable(step.i, step.j) else (230, 126, 34)\n",
    "            draw_rectangle(draw, step, scale, color)\n",
    "\n",
    "    if start is not None and grid_map.traversable(start.i, start.j):\n",
    "        draw_rectangle(draw, start, scale, (40, 180, 99))\n",
    "\n",
    "    if goal is not None and grid_map.traversable(goal.i, goal.j):\n",
    "        draw_rectangle(draw, goal, scale, (231, 76, 60))\n",
    "\n",
    "    _, ax = plt.subplots(dpi=width * scale)\n",
    "    ax.axes.xaxis.set_visible(False)\n",
    "    ax.axes.yaxis.set_visible(False)\n",
    "    plt.imshow(np.asarray(im))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_string_to_cells(cell_str: str):\n",
    "    lines = cell_str.replace(\" \", \"\").split(\"\\n\")\n",
    "\n",
    "    cells = np.array(\n",
    "        [[1 if char == \"@\" else 0 for char in line] for line in lines if line],\n",
    "        dtype=np.int8,\n",
    "    )\n",
    "    return cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(i1: int, j1: int, i2: int, j2: int) -> tp.Union[int, float]:\n",
    "    if abs(i1 - i2) + abs(j1 - j2) == 1:\n",
    "        return 1\n",
    "    elif abs(i1 - i2) == 1 and abs(j1 - j2) == 1:\n",
    "        return 2 ** 0.5\n",
    "    else:\n",
    "        raise ValueError(\"Trying to compute the cost of a non-supported move! ONLY cardinal or diagonal moves are supported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_map(path):\n",
    "    with open(path) as map_file:\n",
    "        map_lines = [line for line in map_file.readlines()][2:-5]\n",
    "        map_str = \"\".join(map_lines)\n",
    "        cells = convert_string_to_cells(map_str)\n",
    "\n",
    "    return cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "    \n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, bilinear=False):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = (DoubleConv(n_channels, 32))\n",
    "        self.down1 = (Down(32, 64))\n",
    "        self.down2 = (Down(64, 128))\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down3 = (Down(128, 256 // factor))\n",
    "\n",
    "        self.up1 = (Up(256, 128 // factor, bilinear))\n",
    "        self.up2 = (Up(128, 64 // factor, bilinear))\n",
    "        self.up3 = (Up(64, 32, bilinear))\n",
    "        self.outc = (OutConv(32, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x = self.up1(x4, x3)\n",
    "        x = self.up2(x, x2)\n",
    "        x = self.up3(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(pred, target, cells):\n",
    "    return nn.MSELoss()(pred[~torch.isinf(target)], target[~torch.isinf(target)])\n",
    "\n",
    "\n",
    "def mae_loss(pred, target, cells):\n",
    "    return nn.L1Loss()(pred[~torch.isinf(target)], target[~torch.isinf(target)])\n",
    "\n",
    "\n",
    "def piecewise_mae_loss(pred, target, cells, min_val, max_val, alpha1=1, alpha2=1):\n",
    "    loss = 0\n",
    "    for p, t in zip(pred[~torch.isinf(target)].flatten(), target[~torch.isinf(target)].flatten()):\n",
    "        if p < min_val:\n",
    "            loss += alpha1 * abs(p - t)\n",
    "        elif p > max_val:\n",
    "            loss += alpha2 * abs(p - t)\n",
    "        else:\n",
    "            loss += abs(p - t)\n",
    "    return loss / len(pred.flatten())\n",
    "\n",
    "\n",
    "def lossgrad_loss(pred, target, fake=None):\n",
    "    kernel = torch.tensor([[1, 0, -1],\n",
    "                            [2, 0, -2],\n",
    "                            [1, 0, -1]], dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    kernel = kernel.to(pred.device)\n",
    "    \n",
    "    pred_grad_x = F.conv2d(pred.unsqueeze(0), kernel, padding=1)\n",
    "    target_grad_x = F.conv2d(target.unsqueeze(0), kernel, padding=1)\n",
    "\n",
    "    sobel_kernel_vertical = kernel.transpose(2, 3)\n",
    "    \n",
    "    pred_grad_y = F.conv2d(pred.unsqueeze(0), sobel_kernel_vertical, padding=1)\n",
    "    target_grad_y = F.conv2d(target.unsqueeze(0), sobel_kernel_vertical, padding=1)\n",
    "    \n",
    "    x_ind = ~torch.isinf(target_grad_x) & ~torch.isnan(target_grad_x)\n",
    "    y_ind = ~torch.isinf(target_grad_y) & ~torch.isnan(target_grad_y)\n",
    "\n",
    "    grad_error_x = torch.abs(pred_grad_x[x_ind] - target_grad_x[x_ind])\n",
    "    grad_error_y = torch.abs(pred_grad_y[y_ind] - target_grad_y[y_ind])\n",
    "\n",
    "    loss = (grad_error_x.sum() + grad_error_y.sum()) / (len(grad_error_x) + len(grad_error_y))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TargetCreator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './TransPath_data/train'\n",
    "files = os.listdir(path)\n",
    "def GetPath(name : int | str):\n",
    "    if type(name) is int:\n",
    "        return np.load(os.path.join(path, files[name]), mmap_mode='c')\n",
    "    else:\n",
    "        return np.load(os.path.join(path, name), mmap_mode='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridData(Dataset):\n",
    "    def __init__(self, path, img_size=64):\n",
    "        self.img_size = img_size\n",
    "\n",
    "        self.maps = torch.tensor(np.load(os.path.join(path, 'maps.npy')))\n",
    "        self.starts = torch.tensor(np.load(os.path.join(path, 'starts.npy')))\n",
    "        self.goals = torch.tensor(np.load(os.path.join(path, 'goals.npy')))\n",
    "        self.targets = torch.tensor(np.load(os.path.join(path, 'abs.npy')))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.maps)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        cells = self.maps[idx, 0]\n",
    "        positions_map = self.goals[idx, 0]\n",
    "        shape = (-1, self.img_size, self.img_size)\n",
    "        target = self.targets[idx, 0]\n",
    "        return cells.reshape(shape).type(torch.float), \\\n",
    "            positions_map.reshape(shape).type(torch.float), \\\n",
    "                target.reshape(shape).type(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = './TransPath_data'\n",
    "batch_size = 4\n",
    "max_epochs = 3000\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 0.0\n",
    "limit_train_batches = 500\n",
    "limit_val_batches = 100\n",
    "proj_name = 'motatasher_HS2'\n",
    "run_name = 'default'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = GridData(\n",
    "        path=f'{dataset_dir}/train',\n",
    "    )\n",
    "val_data = GridData(\n",
    "        path=f'{dataset_dir}/val',\n",
    "    )\n",
    "resolution = (train_data.img_size, train_data.img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "train_dataloader = DataLoader(\n",
    "        train_data, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=True, \n",
    "        num_workers=2,\n",
    "        # num_workers=multiprocessing.cpu_count(), freezes jupyter notebook\n",
    "        pin_memory=True\n",
    "    )\n",
    "val_dataloader = DataLoader(\n",
    "        val_data, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=False, \n",
    "        num_workers=2,\n",
    "        # num_workers=multiprocessing.cpu_count(), freezes jupyter notebook\n",
    "        pin_memory=True\n",
    "    )\n",
    "samples = next(iter(val_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "loss_func = lossgrad_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetLit(L.LightningModule):\n",
    "    def __init__(self, model: nn.Module, mode: str='f', learning_rate: float=1e-4,\n",
    "                 weight_decay: float=0.0, loss = mse_loss) -> None:\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.model = model\n",
    "        self.mode = mode\n",
    "        self.loss = loss\n",
    "        self.k = 64 * 64 if mode == 'h' else 1\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def training_step(\n",
    "        self, batch: tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor], batch_idx: int\n",
    "    ) -> STEP_OUTPUT:\n",
    "        cells, positions_map, target = batch\n",
    "        inputs = torch.cat([cells, positions_map], dim=1)\n",
    "        predictions = self.model(inputs)\n",
    "        loss = self.loss(predictions, target, cells)\n",
    "        self.log(f'train_loss', loss, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(\n",
    "        self, batch: tuple[torch.Tensor, torch.Tensor, torch.Tensor], batch_idx: int\n",
    "    ) -> STEP_OUTPUT | None:\n",
    "        cells, positions_map, target = batch\n",
    "        inputs = torch.cat([cells, positions_map], dim=1)\n",
    "        predictions = self.model(inputs)\n",
    "        loss = self.loss(predictions, target, cells)\n",
    "        self.log(f'val_loss', loss, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self) -> dict[str, tp.Any]:\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, max_lr=self.learning_rate, total_steps=self.trainer.estimated_stepping_batches\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": scheduler\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PathLogger(L.Callback):\n",
    "    def __init__(self, val_batch, num_samples=10, mode='f'):\n",
    "        super().__init__()\n",
    "        cells, positions_map, target = val_batch[:num_samples]\n",
    "        inputs = torch.cat([cells, positions_map], dim=1)\n",
    "        self.val_samples = inputs[:num_samples]\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, lightning_module):\n",
    "        val_samples = self.val_samples.to(device=lightning_module.device)\n",
    "        prediction = lightning_module.model(val_samples)\n",
    "        if lightning_module.mode == 'h':\n",
    "            prediction = prediction * 64 * 64\n",
    "\n",
    "        trainer.logger.experiment.log({\n",
    "            'data': [wandb.Image(x) for x in self.val_samples],\n",
    "            'predictions': [wandb.Image(x) for x in torch.cat([val_samples, prediction], dim=1)]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = GridData(\n",
    "        path=f'{dataset_dir}/train',\n",
    "    )\n",
    "val_data = GridData(\n",
    "        path=f'{dataset_dir}/val',\n",
    "    )\n",
    "resolution = (train_data.img_size, train_data.img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "train_dataloader = DataLoader(\n",
    "        train_data, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=True, \n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "val_dataloader = DataLoader(\n",
    "        val_data, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=False, \n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "samples = next(iter(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/motatasher/miniconda3/envs/HS4/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "Trainer will use only 1 of 7 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=7)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "callback = PathLogger(samples)\n",
    "checkpoints = ModelCheckpoint(dirpath='checkpoints/', filename='{mode}-{epoch}-{timestr}', every_n_epochs=50)\n",
    "wandb_logger = WandbLogger(project=proj_name, name=f'{run_name}_{\"h\"}', log_model='all')\n",
    "\n",
    "model = UNet(2)\n",
    "lit_module = UnetLit(\n",
    "        model=model,\n",
    "        mode='h',\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "trainer = L.Trainer(\n",
    "        logger=wandb_logger,\n",
    "        accelerator=\"auto\",\n",
    "        max_epochs=max_epochs,\n",
    "        deterministic=False,\n",
    "        limit_train_batches=limit_train_batches,\n",
    "        limit_val_batches=limit_val_batches,\n",
    "        callbacks=[callback, checkpoints],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name  | Type | Params | Mode\n",
      "--------------------------------------\n",
      "0 | model | UNet | 1.9 M  | eval\n",
      "--------------------------------------\n",
      "1.9 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 M     Total params\n",
      "7.707     Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "74        Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 2/? [00:00<00:00, 18.16it/s]                 "
     ]
    },
    {
     "ename": "Error",
     "evalue": "torchvision is required to render images",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlit_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "File \u001b[0;32m~/miniconda3/envs/HS4/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:539\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 539\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/HS4/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     50\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/miniconda3/envs/HS4/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:575\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    569\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    571\u001b[0m     ckpt_path,\n\u001b[1;32m    572\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    574\u001b[0m )\n\u001b[0;32m--> 575\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/HS4/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:982\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 982\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    987\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/HS4/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:1024\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1023\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1024\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1026\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/miniconda3/envs/HS4/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:1053\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1050\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1053\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1055\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/HS4/lib/python3.12/site-packages/lightning/pytorch/loops/utilities.py:179\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/HS4/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py:151\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_iteration_done()\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_dataloader_outputs()\n\u001b[0;32m--> 151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_run_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/HS4/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py:291\u001b[0m, in \u001b[0;36m_EvaluationLoop.on_run_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39m_logger_connector\u001b[38;5;241m.\u001b[39m_evaluation_epoch_end()\n\u001b[1;32m    290\u001b[0m \u001b[38;5;66;03m# hook\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_evaluation_epoch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m logged_outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logged_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logged_outputs, []  \u001b[38;5;66;03m# free memory\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;66;03m# include any logged outputs on epoch_end\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/HS4/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py:370\u001b[0m, in \u001b[0;36m_EvaluationLoop._on_evaluation_epoch_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    367\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[1;32m    369\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_test_epoch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_validation_epoch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 370\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_callback_hooks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_lightning_module_hook(trainer, hook_name)\n\u001b[1;32m    373\u001b[0m trainer\u001b[38;5;241m.\u001b[39m_logger_connector\u001b[38;5;241m.\u001b[39mon_epoch_end()\n",
      "File \u001b[0;32m~/miniconda3/envs/HS4/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:222\u001b[0m, in \u001b[0;36m_call_callback_hooks\u001b[0;34m(trainer, hook_name, monitoring_callbacks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(fn):\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Callback]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcallback\u001b[38;5;241m.\u001b[39mstate_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 222\u001b[0m             \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pl_module:\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "Cell \u001b[0;32mIn[22], line 15\u001b[0m, in \u001b[0;36mPathLogger.on_validation_epoch_end\u001b[0;34m(self, trainer, lightning_module)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lightning_module\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     12\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m prediction \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m64\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[1;32m     14\u001b[0m trainer\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mexperiment\u001b[38;5;241m.\u001b[39mlog({\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_samples],\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m'\u001b[39m: [wandb\u001b[38;5;241m.\u001b[39mImage(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat([val_samples, prediction], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m     17\u001b[0m })\n",
      "File \u001b[0;32m~/miniconda3/envs/HS4/lib/python3.12/site-packages/wandb/sdk/data_types/image.py:182\u001b[0m, in \u001b[0;36mImage.__init__\u001b[0;34m(self, data_or_path, mode, caption, grouping, classes, boxes, masks, file_type)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_from_path(data_or_path)\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_from_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_initialization_meta(\n\u001b[1;32m    184\u001b[0m     grouping, caption, classes, boxes, masks, file_type\n\u001b[1;32m    185\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/HS4/lib/python3.12/site-packages/wandb/sdk/data_types/image.py:310\u001b[0m, in \u001b[0;36mImage._initialize_from_data\u001b[0;34m(self, data, mode, file_type)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_image \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m util\u001b[38;5;241m.\u001b[39mis_pytorch_tensor_typename(util\u001b[38;5;241m.\u001b[39mget_full_typename(data)):\n\u001b[0;32m--> 310\u001b[0m     vis_util \u001b[38;5;241m=\u001b[39m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorchvision.utils\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorchvision is required to render images\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_grad\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m data\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[1;32m    314\u001b[0m         data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdetach()  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/HS4/lib/python3.12/site-packages/wandb/util.py:251\u001b[0m, in \u001b[0;36mget_module\u001b[0;34m(name, required, lazy)\u001b[0m\n\u001b[1;32m    249\u001b[0m             logger\u001b[38;5;241m.\u001b[39mexception(msg)\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m required \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _not_importable:\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m wandb\u001b[38;5;241m.\u001b[39mError(required)\n",
      "\u001b[0;31mError\u001b[0m: torchvision is required to render images"
     ]
    }
   ],
   "source": [
    "# trainer.fit(lit_module, train_dataloader, val_dataloader)\n",
    "# wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HS4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
